{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take advantage of the two extra hours in your schedule from not having lectures on Wednesday and Friday this week to make substantial progress on your project.  For Monday, November 4, please list what you proposed to have completed by the end of Week 5, and summarize what you have accomplished.  You can be as detailed as you like.  This counts towards your preliminary report score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed to be done by the end of the week:\n",
    "*  Finish the frequency analysis of live music clips.   \n",
    "    * right now I have done frequency analysis for 1 live music,I need to finish the rest 4 live music.\n",
    "    * also right now my function that seperates each music from the large audio file does not seem to be working, so I need to fix that function so that I can process each music separatly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Have I done so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing a noise dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/minsithu/audio-noise-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.54M/1.54M [00:00<00:00, 13.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/primepi/.cache/kagglehub/datasets/minsithu/audio-noise-dataset/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"minsithu/audio-noise-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting audio file separated from live music video file for audio processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "\n",
    "video_file = 'input_video.mov'\n",
    "audio_output = 'output_audio.wav'\n",
    "\n",
    "# Load the video file\n",
    "video_clip = VideoFileClip(video_file)\n",
    "\n",
    "# Extract audio\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Write the audio to a WAV file\n",
    "audio_clip.write_audiofile(audio_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the one large audio file to mutiple live music songs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting song_1.wav\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = 'output_audio.wav'\n",
    "audio = AudioSegment.from_wav(audio_file)\n",
    "\n",
    "# Parameters for silence detection\n",
    "min_silence_len = 2000  # Minimum length of silence (in ms) to be used for a split\n",
    "silence_thresh = -50    # Silence threshold (in dBFS)\n",
    "\n",
    "# Split audio where silence is 'silent' for at least 'min_silence_len' milliseconds\n",
    "chunks = split_on_silence(\n",
    "    audio,\n",
    "    min_silence_len=min_silence_len,\n",
    "    silence_thresh=silence_thresh\n",
    ")\n",
    "\n",
    "# Export each chunk as a separate audio file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_file = f\"song_{i+1}.wav\"\n",
    "    print(f\"Exporting {chunk_file}\")\n",
    "    chunk.export(chunk_file, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier transform on each song for frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format b'ID3\\x02' not understood. Only 'RIFF' and 'RIFX' supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m song_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msong_*.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m song_file \u001b[38;5;129;01min\u001b[39;00m song_files:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Read the WAV file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     sample_rate, data \u001b[38;5;241m=\u001b[39m wavfile\u001b[38;5;241m.\u001b[39mread(song_file)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# If stereo, take only one channel\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/io/wavfile.py:650\u001b[0m, in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    647\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     file_size, is_big_endian \u001b[38;5;241m=\u001b[39m _read_riff_chunk(fid)\n\u001b[1;32m    651\u001b[0m     fmt_chunk_received \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     data_chunk_received \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/io/wavfile.py:521\u001b[0m, in \u001b[0;36m_read_riff_chunk\u001b[0;34m(fid)\u001b[0m\n\u001b[1;32m    518\u001b[0m     fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>I\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# There are also .wav files with \"FFIR\" or \"XFIR\" signatures?\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(str1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not understood. Only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIFF\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIFX\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# Size of entire file\u001b[39;00m\n\u001b[1;32m    525\u001b[0m file_size \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(fmt, fid\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m8\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: File format b'ID3\\x02' not understood. Only 'RIFF' and 'RIFX' supported."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get a list of all song files\n",
    "song_files = glob.glob('song_*.mp3')\n",
    "\n",
    "for song_file in song_files:\n",
    "    # Read the WAV file\n",
    "    sample_rate, data = wavfile.read(song_file)\n",
    "    \n",
    "    # If stereo, take only one channel\n",
    "    if len(data.shape) > 1:\n",
    "        data = data[:, 0]\n",
    "\n",
    "    # Normalize data\n",
    "    data = data / np.max(np.abs(data))\n",
    "\n",
    "    # Number of samples\n",
    "    N = len(data)\n",
    "\n",
    "    # Compute Fourier Transform\n",
    "    yf = np.fft.fft(data)\n",
    "    xf = np.fft.fftfreq(N, 1 / sample_rate)\n",
    "\n",
    "    # Only take the positive frequencies\n",
    "    idxs = np.where(xf >= 0)\n",
    "    xf = xf[idxs]\n",
    "    yf = np.abs(yf[idxs])\n",
    "\n",
    "    # Plot the frequency spectrum\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(xf, yf)\n",
    "    plt.title(f'Frequency Spectrum of {os.path.basename(song_file)}')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "* Gather clean studio record music of the same song for each live music.\n",
    "* Fourier transform them and get their frequency plot.\n",
    "* Time analysis to align each live music with their clean studio record.\n",
    "* Apply polynomial function that derived from time and frequency analysis of the song to denoise the live music while maintaining its natural form."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
